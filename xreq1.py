import requests
from bs4 import BeautifulSoup
import csv
import os
import time
import re
import pandas as pd
BASE_URL = "https://www.longtermcare.or.kr"
LIST_URL = BASE_URL + "/npbs/cms/board/board/Board.jsp"
ATTACH_DIR = "attachments"
os.makedirs(ATTACH_DIR, exist_ok=True)

HEADERS = {"User-Agent": "Mozilla/5.0"}

def get_board_ids(page):
    params = {
        "act": "LIST",
        "communityKey": "B0022",
        "pageNum": page,
        "pageSize": 10
    }
    res = requests.get(LIST_URL, params=params, headers=HEADERS)
    soup = BeautifulSoup(res.text, "html.parser")
    board_ids = []
    for tag in soup.select("a[href*='boardId=']"):
        href = tag.get("href")
        if "boardId=" in href:
            board_id = href.split("boardId=")[-1].split("&")[0]
            board_ids.append(board_id)
    return list(set(board_ids))

def download_file(file_url, file_name):
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://www.longtermcare.or.kr"
    }
    # íŒŒì¼ í¬ê¸° ì •ë³´ ì œê±° (ì˜ˆ: "íŒŒì¼ëª….pdf (400384 Bytes)")
    file_name = re.sub(r'\s*\(\d+\s*Bytes\)', '', file_name)
    
    
    try:
        res = requests.get(file_url, headers=headers, allow_redirects=True)
        if res.status_code == 200 and len(res.content) > 1000:
            save_path = os.path.join(ATTACH_DIR, file_name)
            with open(save_path, "wb") as f:
                f.write(res.content)
            print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {file_name}")
            return save_path
        else:
            print(f"âš ï¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {file_name} â†’ {res.status_code}")
            return None
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {file_name} â†’ {e}")
        return None



def parse_post(board_id):
    url = f"https://www.longtermcare.or.kr/npbs/cms/board/board/Board.jsp?communityKey=B0022&boardId={board_id}&act=VIEW"
    res = requests.get(url, headers=HEADERS)
    soup = BeautifulSoup(res.text, "html.parser")

    # âœ… ì œëª©
    try:
        title = soup.select_one("div.tbl_tit_wrap span.tbl_tit").text.strip()
    except:
        title = "ì œëª© ì—†ìŒ"

    # âœ… ë³¸ë¬¸
    try:
        content_tag = soup.select_one("td#BOARD_CONTENT")
        # CSV íŒŒì¼ì— ì €ì¥í•  ë•Œ íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ë¥¼ ìœ„í•´ í…ìŠ¤íŠ¸ ì •ë¦¬
        content = content_tag.get_text(separator="\n").strip()
        # ë”°ì˜´í‘œë‚˜ ì‰¼í‘œ ë“± CSV íŒŒì¼ì—ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë¬¸ì ì²˜ë¦¬
        content = content.replace('"', '""')  # í°ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„
    except:
        content = "ë³¸ë¬¸ ì—†ìŒ"

    # âœ… ì²¨ë¶€íŒŒì¼
    attachments = []
    file_section = soup.select_one("td.tongboard_view[colspan='3']")
    if file_section:
        for link in file_section.find_all("a", href=True):
            file_url = BASE_URL + link["href"]
            file_name = link.text.strip()
                   # ê´„í˜¸ ì•ˆì˜ Bytes ì •ë³´ ì œê±°í•˜ê³  í™•ì¥ìê¹Œì§€ í¬í•¨ëœ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
             #match = re.match(r"(.+\.(pdf|hwp))\s*\(.*?\)", raw_file_name, re.IGNORECASE)
            #if match:
            #    file_name = match.group(1).strip()
            #else:
            #    file_name = raw_file_name.strip()  # ì˜ˆì™¸ ìƒí™© ëŒ€ë¹„ ë°±ì—…
            file_name = re.sub(r'\s*\(\d+\s*Bytes\)', '', file_name)
            ext = os.path.splitext(file_name)[-1].lower()
            if ".hwp" in ext or ".pdf" in ext:
                download_file(file_url, file_name)
                attachments.append(f"{file_name} ({file_url})")
    else:
        print("ì²¨ë¶€íŒŒì¼ ì—†ìŒ")
    return {
        "title": title,
        "url": url,
        "content": content,
        "attachments": "; ".join(attachments)
    }

def save_to_csv(data, filename="ë³µì§€ìš©êµ¬_ìë£Œì‹¤.csv"):
    try:
        df = pd.DataFrame(data)
        df.to_csv(filename, index=False, encoding="utf-8-sig", quotechar='"', quoting=csv.QUOTE_ALL, escapechar='\\')
        print(f"ğŸ“Š ì´ {len(data)}ê°œ ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"CSV ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ëŒ€ì•ˆìœ¼ë¡œ JSONìœ¼ë¡œ ì €ì¥
        import json
        with open(filename.replace('.csv', '.json'), 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“Š ì´ {len(data)}ê°œ ë°ì´í„°ê°€ JSON íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
if __name__ == "__main__":
    all_data = []
    for page in range(21, 22):  # 1~21í˜ì´ì§€
        board_ids = get_board_ids(page)
        for board_id in board_ids:
            post = parse_post(board_id)
            all_data.append(post)
            print(f"âœ… ì €ì¥ ëŒ€ìƒ: {post['title']}")
            time.sleep(0.5)

    save_to_csv(all_data)
    print("\nğŸ“ ëª¨ë“  ê²Œì‹œë¬¼ì„ CSV íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ!")
