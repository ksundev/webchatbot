import requests
from bs4 import BeautifulSoup
import csv
import os
import time
import re
import pandas as pd
import os, shutil
import unicodedata
import fitz  # PyMuPDF
import sys
from pathlib import Path

BASE_URL = "https://www.longtermcare.or.kr"
LIST_URL = BASE_URL + "/npbs/cms/board/board/Board.jsp"

ATTACH_DIR = "attachments1"
PDF_DIR = os.path.join(ATTACH_DIR, "pdf")
HWP_DIR = os.path.join(ATTACH_DIR, "hwp")
XLSX_DIR = os.path.join(ATTACH_DIR, "xlsx")
XLS_DIR  = os.path.join(ATTACH_DIR, "xls")
ZIP_DIR  = os.path.join(ATTACH_DIR, "zip")
PDF_TEXT_DIR = os.path.join(PDF_DIR, "text")
PDF_IMAGE_DIR = os.path.join(PDF_DIR, "image")
os.makedirs(ATTACH_DIR, exist_ok=True)
os.makedirs(PDF_DIR, exist_ok=True)
os.makedirs(HWP_DIR, exist_ok=True)
os.makedirs(XLSX_DIR, exist_ok=True)
os.makedirs(XLS_DIR,  exist_ok=True)
os.makedirs(ZIP_DIR,  exist_ok=True)
os.makedirs(PDF_TEXT_DIR,  exist_ok=True)
os.makedirs(PDF_IMAGE_DIR, exist_ok=True)

EXT_DIRS = {
    ".pdf": PDF_DIR,
    ".hwp": HWP_DIR,
    ".xlsx": XLSX_DIR,
    ".xls": XLS_DIR,
    ".zip": ZIP_DIR,
}
ALLOWED_EXTS = set(EXT_DIRS.keys())

# í´ë” ìƒì„±
os.makedirs(ATTACH_DIR, exist_ok=True)
for d in EXT_DIRS.values():
    os.makedirs(d, exist_ok=True)

HEADERS = {"User-Agent": "Mozilla/5.0"}

def ensure_unique_path(dirpath: str, filename: str) -> str:
    base, ext = os.path.splitext(filename)
    candidate = os.path.join(dirpath, filename)
    i = 1
    while os.path.exists(candidate):
        candidate = os.path.join(dirpath, f"{base}_{i}{ext}")
        i += 1
    return candidate

def pdf_has_any_image(pdf_path: str) -> bool:
    """í˜ì´ì§€ ì¤‘ í•˜ë‚˜ë¼ë„ ì´ë¯¸ì§€ XObjectê°€ ìˆìœ¼ë©´ True"""
    try:
        with fitz.open(pdf_path) as doc:
            for i, page in enumerate(doc, start=1):
                imgs = page.get_images(full=True)
                # ë””ë²„ê¹…
                print(f"   - {os.path.basename(pdf_path)} p{i}: images={len(imgs)}")
                if imgs:
                    return True
    except Exception as e:
        print(f"âš ï¸ PDF ì—´ê¸° ì‹¤íŒ¨: {os.path.basename(pdf_path)} â†’ {e}")
    return False

def split_pdf_by_content():
    moved = {"image": 0, "text": 0}
    for fname in os.listdir(PDF_DIR):
        src = os.path.join(PDF_DIR, fname)
        if not os.path.isfile(src):
            continue
        if not fname.lower().endswith(".pdf"):
            continue

        # ì´ë¯¸ì§€ ì—¬ë¶€ íŒì •
        has_img = pdf_has_any_image(src)
        dst_dir = PDF_IMAGE_DIR if has_img else PDF_TEXT_DIR
        
        # ì´ë¯¸ ë™ì¼í•œ ì´ë¦„ì˜ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        dst_file = os.path.join(dst_dir, fname)
        if os.path.exists(dst_file):
            print(f"â­ï¸ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ ê±´ë„ˆëœ€: {fname}")
            continue
            
        dst = ensure_unique_path(dst_dir, fname)
        shutil.move(src, dst)
        moved["image" if has_img else "text"] += 1
        print(f"ğŸ“¦ ì´ë™: {fname}  â†’  {os.path.relpath(dst)}")

    print(f"\nâœ… ì •ë¦¬ ì™„ë£Œ: image {moved['image']}ê°œ, text {moved['text']}ê°œ")

def convert_hwp_to_pdf():
    """ë‹¤ìš´ë¡œë“œëœ HWP íŒŒì¼ë“¤ì„ PDFë¡œ ìë™ ë³€í™˜"""
    print("\nğŸ”„ HWP â†’ PDF ë³€í™˜ ì‹œì‘...")
    
    try:
        import win32com.client as win32
    except Exception:
        print("âš ï¸ pywin32ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. HWP ë³€í™˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        print("   ì„¤ì¹˜ ë°©ë²•: pip install pywin32")
        return
    
    SRC = Path(HWP_DIR)
    DST = Path(PDF_DIR)
    PDF_TEXT = Path(PDF_TEXT_DIR)
    PDF_IMAGE = Path(PDF_IMAGE_DIR)
    
    if not SRC.exists():
        print(f"âš ï¸ HWP í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {SRC}")
        return
    
    # PDF í´ë” ìƒì„±
    DST.mkdir(parents=True, exist_ok=True)
    
    # HWP íŒŒì¼ ëª©ë¡
    hwp_files = list(SRC.rglob("*.hwp"))
    if not hwp_files:
        print("ğŸ“‚ ë³€í™˜í•  HWP íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“‹ ë³€í™˜ ëŒ€ìƒ HWP íŒŒì¼: {len(hwp_files)}ê°œ")
    
    # ê¸°ì¡´ ë³€í™˜ëœ íŒŒì¼ í™•ì¸ (PDF í´ë” + í•˜ìœ„ í´ë” ëª¨ë‘ ê²€ì‚¬)
    existing_pdf_files = set()
    
    # PDF ê¸°ë³¸ í´ë” í™•ì¸
    for pdf_file in DST.glob("*.pdf"):
        existing_pdf_files.add(pdf_file.stem)
    
    # PDF í…ìŠ¤íŠ¸ í´ë” í™•ì¸
    for pdf_file in PDF_TEXT.glob("*.pdf"):
        existing_pdf_files.add(pdf_file.stem)
    
    # PDF ì´ë¯¸ì§€ í´ë” í™•ì¸
    for pdf_file in PDF_IMAGE.glob("*.pdf"):
        existing_pdf_files.add(pdf_file.stem)
    
    print(f"ğŸ“‹ ê¸°ì¡´ ë³€í™˜ëœ PDF íŒŒì¼: {len(existing_pdf_files)}ê°œ")
    
    # ë°©ë²• 1: ì§ì ‘ ë³€í™˜
    try:
        print("ğŸ”§ Hancom HWP COM ê°ì²´ ì‹¤í–‰...")
        hwp = win32.gencache.EnsureDispatch("HWPFrame.HwpObject")
        
        # ë³´ì•ˆ ì„¤ì • í•´ì œ ì‹œë„
        try:
            hwp.RegisterModule("FilePathCheckDLL", "FilePathCheckerModule")
        except Exception:
            pass
        
        try:
            hwp.XHwpWindows.Item(0).Visible = True
        except:
            pass
    except Exception as e:
        print(f"âŒ HWP COM ê°ì²´ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return
    
    total, ok, skip, fail = 0, 0, 0, 0
    
    for src in hwp_files:
        total += 1
        # SRC ê¸°ì¤€ ìƒëŒ€ê²½ë¡œ ìœ ì§€ â†’ DSTì— ê°™ì€ í´ë” êµ¬ì¡°ë¡œ ì €ì¥
        rel = src.relative_to(SRC)
        out = DST / rel.with_suffix(".pdf")
        out.parent.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ëª…(í™•ì¥ì ì œì™¸)ì´ ì´ë¯¸ ë³€í™˜ëœ íŒŒì¼ ëª©ë¡ì— ìˆëŠ”ì§€ í™•ì¸
        if src.stem in existing_pdf_files:
            print(f"â­ï¸ {src.name} (ì´ë¯¸ PDFë¡œ ë³€í™˜ëœ íŒŒì¼ì´ ì¡´ì¬í•¨)")
            skip += 1
            continue
        
        # ì´ë¯¸ ë³€í™˜ëœ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸ (íŒŒì¼ í¬ê¸°ë„ ì²´í¬)
        if out.exists():
            # íŒŒì¼ í¬ê¸°ê°€ 0ë³´ë‹¤ í°ì§€ í™•ì¸ (ì •ìƒì ìœ¼ë¡œ ë³€í™˜ëœ íŒŒì¼ì¸ì§€)
            if out.stat().st_size > 0:
                print(f"â­ï¸ {src.name} (ì´ë¯¸ ë³€í™˜ë¨, {out.stat().st_size:,} bytes)")
                skip += 1
                continue
            else:
                print(f"ğŸ”„ {out.name} íŒŒì¼ì´ ë¹„ì–´ìˆì–´ì„œ ë‹¤ì‹œ ë³€í™˜í•©ë‹ˆë‹¤.")
                out.unlink()  # ë¹ˆ íŒŒì¼ ì‚­ì œ
        
        try:
            print(f"ğŸ”„ ë³€í™˜ ì¤‘: {src.name}")
            
            # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            src_abs = str(src.resolve())
            out_abs = str(out.resolve())
            
            # íŒŒì¼ ì—´ê¸°
            hwp.Open(src_abs)
            
            # ë°©ë²• 1: ì§ì ‘ ë³€í™˜ ì‹œë„
            try:
                hwp.SaveAs(out_abs, "PDF")
                success = True
            except:
                success = False
            
            # ë°©ë²• 1ì´ ì‹¤íŒ¨í•˜ë©´ ë°©ë²• 2 ì‹œë„
            if not success or not out.exists() or out.stat().st_size == 0:
                print("  ë°©ë²• 1 ì‹¤íŒ¨, ë°©ë²• 2 ì‹œë„ ì¤‘...")
                try:
                    hwp.HAction.GetDefault("FileSaveAsPdf", hwp.HParameterSet.HFileOpenSave.HSet)
                    hwp.HParameterSet.HFileOpenSave.filename = out_abs
                    hwp.HParameterSet.HFileOpenSave.Format = "PDF"
                    hwp.HAction.Execute("FileSaveAsPdf", hwp.HParameterSet.HFileOpenSave.HSet)
                except:
                    pass
            
            # íŒŒì¼ í™•ì¸
            if out.exists() and out.stat().st_size > 0:
                print(f"âœ… ë³€í™˜ ì™„ë£Œ: {src.name} â†’ {out.name} ({out.stat().st_size:,} bytes)")
                # ë³€í™˜ ì„±ê³µí•œ íŒŒì¼ ëª©ë¡ì— ì¶”ê°€
                existing_pdf_files.add(src.stem)
                ok += 1
            else:
                print(f"âŒ ë³€í™˜ ì‹¤íŒ¨: {src.name}")
                fail += 1
            
            hwp.Clear(1)  # í˜„ì¬ ë¬¸ì„œ ë‹«ê¸°
        except Exception as e:
            print(f"âŒ ë³€í™˜ ì˜¤ë¥˜ {src.name}: {e}")
            fail += 1
    
    # HWP ì¢…ë£Œ
    try:
        hwp.Quit()
    except Exception:
        pass
    
    print(f"\nğŸ“Š ë³€í™˜ ìš”ì•½: ì´ {total}ê°œ / ë³€í™˜ {ok}ê°œ / ê±´ë„ˆëœ€ {skip}ê°œ / ì‹¤íŒ¨ {fail}ê°œ")
    
    # ê²°ê³¼ í™•ì¸
    pdf_files = list(DST.rglob("*.pdf"))
    print(f"ğŸ“‚ PDF í´ë” ë‚´ ì´ íŒŒì¼ ìˆ˜: {len(pdf_files)}ê°œ")

def sanitize_filename(name: str) -> str:
    # ìœˆë„ìš° ê¸ˆì¹™ë¬¸ì ì œê±° + ì•ë’¤ ê³µë°± ì •ë¦¬
    name = unicodedata.normalize("NFC", name)
    name = re.sub(r'[\\/:*?"<>|]', '_', name).strip()
    # ë„ˆë¬´ ê¸´ íŒŒì¼ëª… ë°©ì§€ (í™•ì¥ì ì œì™¸ 180ì ì •ë„ë¡œ ì»·)
    root, ext = os.path.splitext(name)
    if len(root) > 180:
        root = root[:180]
    return root + ext

def extract_reg_date_prefix(soup: BeautifulSoup) -> str:
    """í˜ì´ì§€ ë‚´ YYYY/MM/DD, YYYY-MM-DD, YYYY.MM.DD â†’ YYYYMMDDë¡œ ë³€í™˜"""
    # ìš°ì„  th.tongboard_viewì—ì„œ ì§ì ‘ ì°¾ê¸° (ë„¤ê°€ ë§í•œ ìœ„ì¹˜)
    for th in soup.find_all("th", class_="tongboard_view"):
        txt = th.get_text(strip=True)
        m = re.search(r'(20\d{2})[./-](\d{1,2})[./-](\d{1,2})', txt)
        if m:
            y, mo, d = m.group(1), int(m.group(2)), int(m.group(3))
            return f"{y}{mo:02d}{d:02d}"
    # ë°±ì—…: í˜ì´ì§€ ì „ì²´ì—ì„œë¼ë„ ì°¾ê¸°
    txt_all = soup.get_text(" ", strip=True)
    m = re.search(r'(20\d{2})[./-](\d{1,2})[./-](\d{1,2})', txt_all)
    if m:
        y, mo, d = m.group(1), int(m.group(2)), int(m.group(3))
        return f"{y}{mo:02d}{d:02d}"
    return "00000000"

def get_board_ids(page):
    params = {
        "act": "LIST",
        "communityKey": "B0018",  # ë³€ê²½ëœ communityKey
        "pageNum": page,
        "pageSize": 10
    }
    res = requests.get(LIST_URL, params=params, headers=HEADERS)
    soup = BeautifulSoup(res.text, "html.parser")
    board_ids = []
    for tag in soup.select("a[href*='boardId=']"):
        href = tag.get("href")
        if "boardId=" in href:
            board_id = href.split("boardId=")[-1].split("&")[0]
            board_ids.append(board_id)
    return list(set(board_ids))

def download_file(file_url, file_name):
    headers = {"User-Agent": "Mozilla/5.0", "Referer": "https://www.longtermcare.or.kr"}

    # "(12345 Bytes)" ê¼¬ë¦¬ ë°©ì–´ + íŒŒì¼ëª… í´ë¦°
    file_name = re.sub(r'\s*\(\d+\s*bytes?\)\s*$', '', file_name, flags=re.IGNORECASE).strip()
    file_name = sanitize_filename(file_name)

    ext = os.path.splitext(file_name)[-1].lower()
    target_dir = EXT_DIRS.get(ext, ATTACH_DIR)
    
    save_path = os.path.join(target_dir, file_name)
    
    # PDF íŒŒì¼ì¸ ê²½ìš° PDF_TEXT_DIRì™€ PDF_IMAGE_DIRì—ë„ ì¤‘ë³µ í™•ì¸
    if ext == ".pdf":
        # PDF ê¸°ë³¸ í´ë”, í…ìŠ¤íŠ¸ í´ë”, ì´ë¯¸ì§€ í´ë” ëª¨ë‘ í™•ì¸
        if (os.path.exists(save_path) or 
            os.path.exists(os.path.join(PDF_TEXT_DIR, file_name)) or 
            os.path.exists(os.path.join(PDF_IMAGE_DIR, file_name))):
            print(f"â­ï¸ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” PDF íŒŒì¼ ê±´ë„ˆëœ€: {file_name}")
            return save_path

    try:
        with requests.get(file_url, headers=headers, allow_redirects=True, timeout=30, stream=True) as r:
            r.raise_for_status()
            total = 0
            # âœ… í•­ìƒ ë®ì–´ì“°ê¸°
            with open(save_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=256 * 1024):
                    if chunk:
                        f.write(chunk)
                        total += len(chunk)

        if total < 1024:  # ë„ˆë¬´ ì‘ìœ¼ë©´ ì‹¤íŒ¨ ì²˜ë¦¬(ì˜µì…˜)
            print(f"âš ï¸ ë‹¤ìš´ë¡œë“œ ì˜ì‹¬(ë„ˆë¬´ ì‘ìŒ): {file_name} size={total} bytes")
            try:
                os.remove(save_path)
            except:
                pass
            return None

        print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {os.path.relpath(save_path)}")
        return save_path

    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {file_name} â†’ {e}")
        return None


def parse_post(board_id):
    url = f"https://www.longtermcare.or.kr/npbs/cms/board/board/Board.jsp?communityKey=B0018&boardId={board_id}&act=VIEW"
    res = requests.get(url, headers=HEADERS)
    soup = BeautifulSoup(res.text, "html.parser")

    # âœ… ì œëª©
    try:
        title = soup.select_one("div.tbl_tit_wrap span.tbl_tit").text.strip()
    except:
        title = "ì œëª© ì—†ìŒ"

    # âœ… ë³¸ë¬¸
    try:
        content_tag = soup.select_one("td#BOARD_CONTENT")
        # CSV íŒŒì¼ì— ì €ì¥í•  ë•Œ íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ë¥¼ ìœ„í•´ í…ìŠ¤íŠ¸ ì •ë¦¬
        content = content_tag.get_text(separator="\n").strip()
        # ë”°ì˜´í‘œë‚˜ ì‰¼í‘œ ë“± CSV íŒŒì¼ì—ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë¬¸ì ì²˜ë¦¬
        content = content.replace('"', '""')  # í°ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„
    except:
        content = "ë³¸ë¬¸ ì—†ìŒ"

    # âœ… ë“±ë¡ì¼ í”„ë¦¬í”½ìŠ¤ (ì˜ˆ: 20250430)
    reg_prefix = extract_reg_date_prefix(soup)

    # âœ… ì²¨ë¶€íŒŒì¼
    attachments = []
    file_section = soup.select_one("td.tongboard_view[colspan='3']")
    if file_section:
        for link in file_section.find_all("a", href=True):
            file_url = BASE_URL + link["href"]
            raw_name = link.text.strip()
            # "(12345 Bytes)" ê¼¬ë¦¬ ì œê±°
            clean_name = re.sub(r'\s*\(\d+\s*Bytes\)\s*$', '', raw_name).strip()
            ext = os.path.splitext(clean_name)[-1].lower()

            if ext in ALLOWED_EXTS:
                final_name = f"{reg_prefix}{clean_name}"   # ë‚ ì§œ í”„ë¦¬í”½ìŠ¤ ìœ ì§€
                download_file(file_url, final_name)
                attachments.append(f"{final_name} ({file_url})")

    else:
        print("ì²¨ë¶€íŒŒì¼ ì—†ìŒ")

    return {
        "title": title,
        "url": url,
        "content": content,
        "reg_date": reg_prefix,                 # â† CSVì—ë„ ë“±ë¡ì¼ ë„£ìŒ
        "attachments": "; ".join(attachments)
    }

def save_to_csv(data, filename="ë³µì§€ìš©êµ¬_ë²•ë ¹ìë£Œì‹¤.csv"):
    try:
        df = pd.DataFrame(data)
        df.to_csv(filename, index=False, encoding="utf-8-sig", quotechar='"', quoting=csv.QUOTE_ALL, escapechar='\\')
        print(f"ğŸ“Š ì´ {len(data)}ê°œ ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"CSV ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ëŒ€ì•ˆìœ¼ë¡œ JSONìœ¼ë¡œ ì €ì¥
        import json
        with open(filename.replace('.csv', '.json'), 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“Š ì´ {len(data)}ê°œ ë°ì´í„°ê°€ JSON íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
if __name__ == "__main__":
    all_data = []
    for page in range(1, 2):  # 1~5í˜ì´ì§€
        board_ids = get_board_ids(page)
        for board_id in board_ids:
            post = parse_post(board_id)
            all_data.append(post)
            print(f"âœ… ì €ì¥ ëŒ€ìƒ: {post['title']}")
            time.sleep(0.5)

    save_to_csv(all_data)
    print("\nğŸ“ ëª¨ë“  ê²Œì‹œë¬¼ì„ CSV íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ!")
    
    # ğŸ”„ HWP â†’ PDF ìë™ ë³€í™˜
    convert_hwp_to_pdf()
    
    # ğŸ“‚ PDF ìë™ ë¶„ë¥˜
    split_pdf_by_content()





